{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 1000])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Load the saved dataloader and logits\n",
    "model_name = 'ViT-B-32-quickgelu'\n",
    "model_name = 'ViT-H-14-378-quickgelu'\n",
    "logits = torch.load(f'total_logits_{model_name}.pt')\n",
    "device = logits.device\n",
    "targets = torch.load(f'total_targets_{model_name}.pt')\n",
    "\n",
    "print(logits.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-1 Accuracy: 0.8436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute the accuracy\n",
    "\n",
    "with torch.no_grad(): # No need to track gradients for accuracy calculation\n",
    "    # Get the index of the max logit score (predicted class)\n",
    "    _, predicted_classes = torch.max(logits, dim=1)\n",
    "\n",
    "    # Compare predicted classes with the true targets\n",
    "    correct_predictions = (predicted_classes == targets)\n",
    "\n",
    "    # Calculate the mean accuracy\n",
    "    top1_accuracy = correct_predictions.float().mean()\n",
    "    print(f\"\\nTop-1 Accuracy: {top1_accuracy.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 0.6666797399520874, std = 0.0184860210865736, min = 0.0, max = 0.0633256733417511, acc = 0.8437199592590332\n",
      "Iteration 1000: Loss = 0.636522650718689, std = 0.1741292029619217, min = 0.0, max = 1.6848067045211792, acc = 0.8478800058364868\n",
      "Iteration 2000: Loss = 0.6164097785949707, std = 0.31445828080177307, min = 0.0, max = 3.0328052043914795, acc = 0.8502799868583679\n",
      "Iteration 3000: Loss = 0.6022956371307373, std = 0.43126922845840454, min = 0.0, max = 4.116040229797363, acc = 0.8511599898338318\n",
      "Iteration 4000: Loss = 0.5920002460479736, std = 0.5300028324127197, min = 0.0, max = 5.026841640472412, acc = 0.8521999716758728\n",
      "Iteration 5000: Loss = 0.5842309594154358, std = 0.6146802306175232, min = 0.0, max = 5.777279853820801, acc = 0.852620005607605\n",
      "Iteration 6000: Loss = 0.5781932473182678, std = 0.6882807016372681, min = 0.0, max = 6.4720234870910645, acc = 0.8532599806785583\n",
      "Iteration 7000: Loss = 0.5733891129493713, std = 0.7529970407485962, min = 0.0, max = 7.072044372558594, acc = 0.8536399602890015\n",
      "Iteration 8000: Loss = 0.5694950222969055, std = 0.8104457855224609, min = 0.0, max = 7.594639301300049, acc = 0.8540399670600891\n",
      "Iteration 9000: Loss = 0.5662919282913208, std = 0.8618375658988953, min = 0.0, max = 8.052188873291016, acc = 0.854479968547821\n",
      "Iteration 10000: Loss = 0.5636239647865295, std = 0.9081087112426758, min = 0.0, max = 8.454036712646484, acc = 0.8548199534416199\n",
      "Iteration 11000: Loss = 0.5613755583763123, std = 0.950009822845459, min = 0.0, max = 8.80789852142334, acc = 0.8547599911689758\n",
      "Iteration 12000: Loss = 0.5594595074653625, std = 0.9881588220596313, min = 0.0, max = 9.120495796203613, acc = 0.855139970779419\n",
      "Iteration 13000: Loss = 0.5578088760375977, std = 1.0230748653411865, min = 0.0, max = 9.397757530212402, acc = 0.8553799986839294\n",
      "Iteration 14000: Loss = 0.5563725829124451, std = 1.0551950931549072, min = 0.0, max = 9.644798278808594, acc = 0.8553799986839294\n",
      "Iteration 15000: Loss = 0.5551115870475769, std = 1.0848875045776367, min = 0.0, max = 9.865915298461914, acc = 0.8553599715232849\n",
      "Iteration 16000: Loss = 0.553995668888092, std = 1.1124601364135742, min = 0.0, max = 10.158831596374512, acc = 0.8554799556732178\n",
      "Iteration 17000: Loss = 0.5530012249946594, std = 1.1381715536117554, min = 0.0, max = 10.43980598449707, acc = 0.8556199669837952\n",
      "Iteration 18000: Loss = 0.5521098971366882, std = 1.162236213684082, min = 0.0, max = 10.695141792297363, acc = 0.8558399677276611\n",
      "Iteration 19000: Loss = 0.5513066649436951, std = 1.184840202331543, min = 0.0, max = 10.927006721496582, acc = 0.85589998960495\n",
      "Iteration 20000: Loss = 0.5505794286727905, std = 1.206134557723999, min = 0.0, max = 11.13742446899414, acc = 0.8558399677276611\n",
      "Iteration 21000: Loss = 0.549918532371521, std = 1.2262507677078247, min = 0.0, max = 11.328314781188965, acc = 0.8557999730110168\n",
      "Iteration 22000: Loss = 0.5493155717849731, std = 1.2452996969223022, min = 0.0, max = 11.501493453979492, acc = 0.8558599948883057\n",
      "Iteration 23000: Loss = 0.5487635731697083, std = 1.2633758783340454, min = 0.0, max = 11.658615112304688, acc = 0.8559199571609497\n",
      "Iteration 24000: Loss = 0.5482567548751831, std = 1.28056800365448, min = 0.0, max = 11.801265716552734, acc = 0.8560599684715271\n",
      "Iteration 25000: Loss = 0.5477901697158813, std = 1.2969416379928589, min = 0.0, max = 11.930835723876953, acc = 0.8560400009155273\n",
      "Iteration 26000: Loss = 0.5473596453666687, std = 1.3125674724578857, min = 0.0, max = 12.04861831665039, acc = 0.8560799956321716\n",
      "Iteration 27000: Loss = 0.5469613075256348, std = 1.327500581741333, min = 0.0, max = 12.155807495117188, acc = 0.8561399579048157\n",
      "Iteration 28000: Loss = 0.5465918779373169, std = 1.341786503791809, min = 0.0, max = 12.253475189208984, acc = 0.85617995262146\n",
      "Iteration 29000: Loss = 0.5462486147880554, std = 1.3554803133010864, min = 0.0, max = 12.342561721801758, acc = 0.8560199737548828\n",
      "Iteration 30000: Loss = 0.5459290146827698, std = 1.3686106204986572, min = 0.0, max = 12.423894882202148, acc = 0.8559399843215942\n",
      "Iteration 31000: Loss = 0.5456312298774719, std = 1.3812103271484375, min = 0.0, max = 12.498296737670898, acc = 0.8559399843215942\n",
      "Iteration 32000: Loss = 0.5453525185585022, std = 1.393332600593567, min = 0.0, max = 12.5664701461792, acc = 0.8558799624443054\n",
      "Iteration 33000: Loss = 0.5450920462608337, std = 1.4049798250198364, min = 0.0, max = 12.628969192504883, acc = 0.8560000061988831\n",
      "Iteration 34000: Loss = 0.5448475480079651, std = 1.4162064790725708, min = 0.0, max = 12.68648624420166, acc = 0.8559799790382385\n",
      "Iteration 35000: Loss = 0.5446179509162903, std = 1.427022933959961, min = 0.0, max = 12.73926067352295, acc = 0.8560400009155273\n",
      "Iteration 36000: Loss = 0.5444020628929138, std = 1.4374622106552124, min = 0.0, max = 12.787991523742676, acc = 0.8561599850654602\n",
      "Iteration 37000: Loss = 0.544198215007782, std = 1.4475464820861816, min = 0.0, max = 12.833001136779785, acc = 0.8562799692153931\n",
      "Iteration 38000: Loss = 0.5440064072608948, std = 1.4572699069976807, min = 0.0, max = 12.874543190002441, acc = 0.8562600016593933\n",
      "Iteration 39000: Loss = 0.5438248515129089, std = 1.4666820764541626, min = 0.0, max = 12.912983894348145, acc = 0.856220006942749\n",
      "Iteration 40000: Loss = 0.5436533093452454, std = 1.4757877588272095, min = 0.0, max = 12.948630332946777, acc = 0.8561599850654602\n",
      "Iteration 41000: Loss = 0.5434904098510742, std = 1.4846057891845703, min = 0.0, max = 12.981790542602539, acc = 0.8561599850654602\n",
      "Iteration 42000: Loss = 0.5433357954025269, std = 1.4931576251983643, min = 0.0, max = 13.012712478637695, acc = 0.8561999797821045\n",
      "Iteration 43000: Loss = 0.5431893467903137, std = 1.501416802406311, min = 0.0, max = 13.041150093078613, acc = 0.8562399744987488\n",
      "Iteration 44000: Loss = 0.5430497527122498, std = 1.5094566345214844, min = 0.0, max = 13.06796646118164, acc = 0.8562799692153931\n",
      "Iteration 45000: Loss = 0.5429167747497559, std = 1.517264485359192, min = 0.0, max = 13.092811584472656, acc = 0.8561999797821045\n",
      "Iteration 46000: Loss = 0.5427902936935425, std = 1.524817943572998, min = 0.0, max = 13.116083145141602, acc = 0.856220006942749\n",
      "Iteration 47000: Loss = 0.5426693558692932, std = 1.5321747064590454, min = 0.0, max = 13.137720108032227, acc = 0.8563599586486816\n",
      "Iteration 48000: Loss = 0.5425539612770081, std = 1.5393333435058594, min = 0.0, max = 13.158270835876465, acc = 0.8563199639320374\n",
      "Iteration 49000: Loss = 0.5424434542655945, std = 1.5463099479675293, min = 0.0, max = 13.177136421203613, acc = 0.8562799692153931\n",
      "Iteration 50000: Loss = 0.5423376560211182, std = 1.553137183189392, min = 0.0, max = 13.195256233215332, acc = 0.8563599586486816\n",
      "Iteration 51000: Loss = 0.542236864566803, std = 1.5596842765808105, min = 0.0, max = 13.211881637573242, acc = 0.8563399910926819\n",
      "Iteration 52000: Loss = 0.5421399474143982, std = 1.5661159753799438, min = 0.0, max = 13.227462768554688, acc = 0.8563599586486816\n",
      "Iteration 53000: Loss = 0.5420464277267456, std = 1.5724431276321411, min = 0.0, max = 13.242595672607422, acc = 0.8562999963760376\n",
      "Iteration 54000: Loss = 0.5419575572013855, std = 1.5785167217254639, min = 0.0, max = 13.256457328796387, acc = 0.8563599586486816\n",
      "Iteration 55000: Loss = 0.5418716073036194, std = 1.5844974517822266, min = 0.0, max = 13.269428253173828, acc = 0.8563799858093262\n",
      "Iteration 56000: Loss = 0.5417894721031189, std = 1.5903091430664062, min = 0.0, max = 13.28182601928711, acc = 0.8563799858093262\n",
      "Iteration 57000: Loss = 0.5417097806930542, std = 1.596017599105835, min = 0.0, max = 13.293837547302246, acc = 0.8563799858093262\n",
      "Iteration 58000: Loss = 0.5416336059570312, std = 1.6015489101409912, min = 0.0, max = 13.304567337036133, acc = 0.8563799858093262\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "lr = 1\n",
    "max_iter = 100000\n",
    "log_iter = 1000\n",
    "loss_his = torch.zeros(max_iter)\n",
    "max_his = torch.zeros(max_iter//log_iter) \n",
    "acc_his = torch.zeros(max_iter//log_iter)\n",
    "\n",
    "\n",
    "# Initialize the parameter outside the loop to avoid reinitialization in each iteration\n",
    "param = nn.Linear(logits.shape[1], 1).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for iter in range(max_iter):\n",
    "    # Compute the output\n",
    "    output = loss_fn(logits + param.weight, targets)\n",
    "    \n",
    "    # Compute gradients\n",
    "    param.zero_grad()  # Clear previous gradients\n",
    "    output.backward()  # Backpropagation\n",
    "\n",
    "    loss_his[iter] = output.item()\n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        param.weight -= lr * param.weight.grad\n",
    "        param.weight -= torch.min(param.weight)  # Ensure non-negative weights\n",
    "    \n",
    "    if iter % log_iter == 0:\n",
    "        # top 1-acc\n",
    "        _, pred = torch.max(logits + param.weight, 1)\n",
    "        acc = (pred == targets).float().mean()\n",
    "        max_his[iter//log_iter] = param.weight.max().item()\n",
    "        acc_his[iter//log_iter] = acc.item()\n",
    "        print(f\"Iteration {iter}: Loss = {output.item()}, std = {param.weight.std().item()}, min = {param.weight.min().item()}, max = {param.weight.max().item()}, acc = {acc.item()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights: Parameter containing:\n",
      "tensor([[2.3996, 3.1800, 2.9355, 0.0000, 2.1533, 2.5203, 2.4460, 2.4669, 3.9907,\n",
      "         2.6469, 1.9435, 1.1919, 5.5982, 3.4883, 2.6283, 1.8619, 6.5320, 1.4669,\n",
      "         1.6356, 2.7517]], device='cuda:0', requires_grad=True)\n",
      "mean logits tensor(9.1875, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final weights:\", param.weight)\n",
    "print(\"mean logits\", logits.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
